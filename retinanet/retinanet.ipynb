{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection using TAO RetinaNet\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Take a pretrained resnet18 model and train a ResNet-18 RetinaNet model on the KITTI dataset\n",
    "* Prune the trained retinanet model\n",
    "* Retrain the pruned model to recover lost accuracy\n",
    "* Export the pruned model\n",
    "* Quantize the pruned model using QAT\n",
    "* Run Inference on the trained model\n",
    "* Export the pruned, quantized and retrained model to a .etlt file for deployment to DeepStream\n",
    "* Run inference on the exported. etlt model to verify deployment using TensorRT\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "This notebook shows an example usecase of RetinaNet object detection using Train Adapt Optimize (TAO) Toolkit.\n",
    "\n",
    "0. [Set up env variables and map drives](#head-0)\n",
    "1. [Install the TAO launcher](#head-1)\n",
    "2. [Prepare dataset and pre-trained model](#head-2) <br>\n",
    "    2.1 [Download the dataset](#head-2-1) <br>\n",
    "    2.2 [Validate the downloaded dataset](#head-2-2) <br>\n",
    "    2.3 [Generate tfrecords from kitti format dataset](#head-2-3) <br>\n",
    "    2.4 [Download pre-trained model](#head-2-4) <br>\n",
    "3. [Provide training specification](#head-3)\n",
    "4. [Run TAO training](#head-4)\n",
    "5. [Evaluate trained models](#head-5)\n",
    "6. [Prune trained models](#head-6)\n",
    "7. [Retrain pruned models](#head-7)\n",
    "8. [Evaluate retrained model](#head-8)\n",
    "9. [Visualize inferences](#head-9)\n",
    "10. [Model Export](#head-10)\n",
    "11. [Verify deployed model](#head-11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up env variables <a class=\"anchor\" id=\"head-0\"></a>\n",
    "\n",
    "When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n",
    "\n",
    "The following notebook requires the user to set an env variable called the `$LOCAL_PROJECT_DIR` as the path to the users workspace. Please note that the dataset to run this notebook is expected to reside in the `$LOCAL_PROJECT_DIR/data`, while the TAO experiment generated collaterals will be output to `$LOCAL_PROJECT_DIR/retinanet`. More information on how to set up the dataset and the supported steps in the TAO workflow are provided in the subsequent cells.\n",
    "\n",
    "*Note: Please make sure to remove any stray artifacts/files from the `$USER_EXPERIMENT_DIR` or `$DATA_DOWNLOAD_DIR` paths as mentioned below, that may have been generated from previous experiments. Having checkpoint files etc may interfere with creating a training graph for a new experiment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please replace the variable with your key.\n",
      "env: KEY=YOUR_KEY\n",
      "env: USER_EXPERIMENT_DIR=/workspace/tao-experiments/retinanet\n",
      "env: DATA_DOWNLOAD_DIR=/workspace/tao-experiments/data\n",
      "env: SPECS_DIR=/workspace/tao-experiments/retinanet/specs\n",
      "total 28\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 1880 Aug 24 23:51 retinanet_train_resnet18_kitti_seq.txt\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 1884 Aug 24 23:51 retinanet_retrain_resnet18_kitti_seq.txt\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 1914 Aug 24 23:51 retinanet_retrain_resnet18_kitti.txt\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  608 Oct 22 14:09 retinanet_tfrecords_kitti_train.txt\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  604 Oct 22 15:21 retinanet_tfrecords_real_kitti_train.txt\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 1856 Oct 23 06:28 retinanet_train_fine_resnet18_kitti.txt\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 1874 Nov 11 12:22 retinanet_train_resnet18_kitti.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Setting up env variables for cleaner command line commands.\n",
    "import os\n",
    "\n",
    "print(\"Please replace the variable with your key.\")\n",
    "%env KEY=YOUR_KEY\n",
    "%env USER_EXPERIMENT_DIR=/workspace/tao-experiments/retinanet\n",
    "%env DATA_DOWNLOAD_DIR=/workspace/tao-experiments/data\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/retinanet\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "# The dataset expected to be present in $LOCAL_PROJECT_DIR/data, while the results for the steps\n",
    "# in this notebook will be stored at $LOCAL_PROJECT_DIR/retinanet\n",
    "os.environ[\"LOCAL_PROJECT_DIR\"] = '/home/ubuntu/cv_samples_v1.2.0/retinanet'\n",
    "os.environ[\"LOCAL_DATA_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"data\")\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"] = os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()), \"retinanet\")\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "os.environ[\"LOCAL_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")\n",
    "%env SPECS_DIR=/workspace/tao-experiments/retinanet/specs\n",
    "\n",
    "# Showing list of specification files.\n",
    "!ls -rlt $LOCAL_SPECS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below maps the project directory on your local host to a workspace directory in the TAO docker instance, so that the data and the results are mapped from in and out of the docker. For more information please refer to the [launcher instance](https://docs.nvidia.com/tao/tao-toolkit/tao_launcher.html) in the user guide.\n",
    "\n",
    "When running this cell on AWS, update the drive_map entry with the dictionary defined below, so that you don't have permission issues when writing data into folders created by the TAO docker.\n",
    "\n",
    "```json\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "            # Mapping the data directory\n",
    "            {\n",
    "                \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "                \"destination\": \"/workspace/tao-experiments\"\n",
    "            },\n",
    "            # Mapping the specs directory.\n",
    "            {\n",
    "                \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "                \"destination\": os.environ[\"SPECS_DIR\"]\n",
    "            },\n",
    "        ],\n",
    "    \"DockerOptions\": {\n",
    "        \"user\": \"{}:{}\".format(os.getuid(), os.getgid())\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "\n",
    "# Define the dictionary with the mapped drives\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": \"/workspace/tao-experiments\"\n",
    "        },\n",
    "        # Mapping the specs directory.\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "            \"destination\": os.environ[\"SPECS_DIR\"]\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"Mounts\": [\r\n",
      "        {\r\n",
      "            \"source\": \"/home/ubuntu/cv_samples_v1.2.0/retinanet\",\r\n",
      "            \"destination\": \"/workspace/tao-experiments\"\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"source\": \"/home/ubuntu/cv_samples_v1.2.0/retinanet/specs\",\r\n",
      "            \"destination\": \"/workspace/tao-experiments/retinanet/specs\"\r\n",
      "        }\r\n",
      "    ]\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of the TAO Toolkit Instance\r\n",
      "dockers: ['nvidia/tao/tao-toolkit-tf', 'nvidia/tao/tao-toolkit-pyt', 'nvidia/tao/tao-toolkit-lm']\r\n",
      "format_version: 1.0\r\n",
      "toolkit_version: 3.21.08\r\n",
      "published_date: 08/17/2021\r\n"
     ]
    }
   ],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Download and Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data folder inside the root directory and download the synthetic and real datasets from the provided links.\n",
    "\n",
    "Dataset is already formatted into train and test for both synthetic and real cases. \n",
    "\n",
    "We have 2 scenarios in the dataset i.e dataset with less complex background and dataset with complex background.\n",
    "\n",
    "\n",
    "Following directory structure will be there according to the spec files. \n",
    "\n",
    "├── real_test\n",
    "├── real_train_20\n",
    "├── synthetic_test\n",
    "├── synthetic_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Prepare tfrecords from kitti format dataset <a class=\"anchor\" id=\"head-2-3\"></a>\n",
    "\n",
    "* Update the tfrecords spec file to take in your kitti format dataset\n",
    "* Create the tfrecords using the dataset_convert \n",
    "* TFRecords only need to be generated once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\r\n",
      "drwxrwxr-x 6 ubuntu ubuntu 4096 Oct 23 10:53 real_test\r\n",
      "drwxrwxr-x 4 ubuntu ubuntu 4096 Oct 17 01:41 real_train_20\r\n",
      "drwxrwxr-x 4 ubuntu ubuntu 4096 Oct 16 18:47 synthetic_test\r\n",
      "drwxrwxr-x 4 ubuntu ubuntu 4096 Oct 16 18:20 synthetic_train\r\n",
      "drwxrwxr-x 4 ubuntu ubuntu 4096 Oct 16 18:20 synthetic_train1\r\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "!ls -l $LOCAL_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFRecords conversion spec file:\n",
      "kitti_config {\r\n",
      "  root_directory_path: \"/workspace/tao-experiments/data/\"\r\n",
      "  image_dir_name: \"synthetic_train/images\"\r\n",
      "  label_dir_name: \"synthetic_train/labels\"\r\n",
      "  image_extension: \".png\"\r\n",
      "  partition_mode: \"random\"\r\n",
      "  num_partitions: 2\r\n",
      "  val_split: 0\r\n",
      "  num_shards: 10\r\n",
      "}\r\n",
      "image_directory_path: \"/workspace/tao-experiments/data/\"\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"screw1\"\r\n",
      "      value: \"screw1\"\r\n",
      "  }\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"screw3\"\r\n",
      "      value: \"screw3\"\r\n",
      "  }\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"screw6\"\r\n",
      "      value: \"screw6\"\r\n",
      "  }\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"screw8\"\r\n",
      "      value: \"screw8\"\r\n",
      "  }\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "print(\"TFRecords conversion spec file:\")\n",
    "!cat $LOCAL_SPECS_DIR/retinanet_tfrecords_kitti_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the training set to TFRecords.\n",
      "2021-11-11 12:26:43,180 [INFO] root: Registry: ['nvcr.io']\n",
      "2021-11-11 12:26:43,259 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/ubuntu/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "2021-11-11 12:26:50,532 - iva.detectnet_v2.dataio.build_converter - INFO - Instantiating a kitti converter\n",
      "2021-11-11 12:26:50,556 - iva.detectnet_v2.dataio.kitti_converter_lib - INFO - Num images in\n",
      "Train: 10661\tVal: 0\n",
      "2021-11-11 12:26:50,556 - iva.detectnet_v2.dataio.kitti_converter_lib - INFO - Skipped validation data...\n",
      "2021-11-11 12:26:50,561 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 0\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/dataset_converter_lib.py:142: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "2021-11-11 12:26:50,561 - tensorflow - WARNING - From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/dataset_converter_lib.py:142: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/iva/detectnet_v2/dataio/kitti_converter_lib.py:283: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "2021-11-11 12:26:54,631 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 1\n",
      "2021-11-11 12:26:58,633 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 2\n",
      "2021-11-11 12:27:02,810 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 3\n",
      "2021-11-11 12:27:08,740 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 4\n",
      "2021-11-11 12:27:12,784 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 5\n",
      "2021-11-11 12:27:16,756 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 6\n",
      "2021-11-11 12:27:21,340 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 7\n",
      "2021-11-11 12:27:23,764 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 8\n",
      "2021-11-11 12:27:26,603 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 9\n",
      "2021-11-11 12:27:31,325 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - \n",
      "Wrote the following numbers of objects:\n",
      "\n",
      "2021-11-11 12:27:31,325 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Cumulative object statistics\n",
      "2021-11-11 12:27:31,325 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - \n",
      "Wrote the following numbers of objects:\n",
      "\n",
      "2021-11-11 12:27:31,326 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Class map. \n",
      "Label in GT: Label in tfrecords file \n",
      "b'screw1': b'screw1'\n",
      "b'screw3': b'screw3'\n",
      "b'screw6': b'screw6'\n",
      "b'screw8': b'screw8'\n",
      "For the dataset_config in the experiment_spec, please use labels in the tfrecords file, while writing the classmap.\n",
      "\n",
      "2021-11-11 12:27:31,326 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Tfrecords generation complete.\n",
      "2021-11-11 12:27:36,988 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# Creating a new directory for the output tfrecords dump.\n",
    "print(\"Converting the training set to TFRecords.\")\n",
    "!mkdir -p $LOCAL_DATA_DIR/tfrecords && rm -rf $LOCAL_DATA_DIR/tfrecords/*\n",
    "!tao retinanet dataset_convert \\\n",
    "               -d $SPECS_DIR/retinanet_tfrecords_kitti_train.txt \\\n",
    "               -o $DATA_DOWNLOAD_DIR/tfrecords/kitti_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the training set to TFRecords.\n",
      "2021-11-11 12:27:47,832 [INFO] root: Registry: ['nvcr.io']\n",
      "2021-11-11 12:27:47,961 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/ubuntu/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "2021-11-11 12:27:55,079 - iva.detectnet_v2.dataio.build_converter - INFO - Instantiating a kitti converter\n",
      "2021-11-11 12:27:55,080 - iva.detectnet_v2.dataio.kitti_converter_lib - INFO - Num images in\n",
      "Train: 195\tVal: 0\n",
      "2021-11-11 12:27:55,081 - iva.detectnet_v2.dataio.kitti_converter_lib - INFO - Skipped validation data...\n",
      "2021-11-11 12:27:55,081 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 0\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/dataset_converter_lib.py:142: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "2021-11-11 12:27:55,081 - tensorflow - WARNING - From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/dataset_converter_lib.py:142: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/iva/detectnet_v2/dataio/kitti_converter_lib.py:283: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "2021-11-11 12:27:55,145 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 1\n",
      "2021-11-11 12:27:55,198 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 2\n",
      "2021-11-11 12:27:55,249 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 3\n",
      "2021-11-11 12:27:55,299 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 4\n",
      "2021-11-11 12:27:55,351 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 5\n",
      "2021-11-11 12:27:55,399 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 6\n",
      "2021-11-11 12:27:55,443 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 7\n",
      "2021-11-11 12:27:55,492 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 8\n",
      "2021-11-11 12:27:55,541 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 9\n",
      "2021-11-11 12:27:55,605 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - \n",
      "Wrote the following numbers of objects:\n",
      "\n",
      "2021-11-11 12:27:55,606 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Cumulative object statistics\n",
      "2021-11-11 12:27:55,606 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - \n",
      "Wrote the following numbers of objects:\n",
      "\n",
      "2021-11-11 12:27:55,606 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Class map. \n",
      "Label in GT: Label in tfrecords file \n",
      "b'screw8': b'screw8'\n",
      "b'screw6': b'screw6'\n",
      "b'screw3': b'screw3'\n",
      "b'screw1': b'screw1'\n",
      "For the dataset_config in the experiment_spec, please use labels in the tfrecords file, while writing the classmap.\n",
      "\n",
      "2021-11-11 12:27:55,606 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Tfrecords generation complete.\n",
      "2021-11-11 12:27:56,600 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# Creating a new directory for the output tfrecords dump.\n",
    "print(\"Converting the training set to TFRecords.\")\n",
    "# !mkdir -p $LOCAL_DATA_DIR/tfrecords && rm -rf $LOCAL_DATA_DIR/tfrecords/*\n",
    "!tao retinanet dataset_convert \\\n",
    "               -d $SPECS_DIR/retinanet_tfrecords_real_kitti_train.txt \\\n",
    "               -o $DATA_DOWNLOAD_DIR/tfrecords/kitti_train_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3729960\r\n",
      "-rw-r--r-- 1 root root 376880632 Nov 11 12:26 kitti_train-fold-000-of-002-shard-00000-of-00010\r\n",
      "-rw-r--r-- 1 root root 381808035 Nov 11 12:26 kitti_train-fold-000-of-002-shard-00001-of-00010\r\n",
      "-rw-r--r-- 1 root root 377996527 Nov 11 12:27 kitti_train-fold-000-of-002-shard-00002-of-00010\r\n",
      "-rw-r--r-- 1 root root 379597772 Nov 11 12:27 kitti_train-fold-000-of-002-shard-00003-of-00010\r\n",
      "-rw-r--r-- 1 root root 384238153 Nov 11 12:27 kitti_train-fold-000-of-002-shard-00004-of-00010\r\n",
      "-rw-r--r-- 1 root root 380008862 Nov 11 12:27 kitti_train-fold-000-of-002-shard-00005-of-00010\r\n",
      "-rw-r--r-- 1 root root 379196557 Nov 11 12:27 kitti_train-fold-000-of-002-shard-00006-of-00010\r\n",
      "-rw-r--r-- 1 root root 382253015 Nov 11 12:27 kitti_train-fold-000-of-002-shard-00007-of-00010\r\n",
      "-rw-r--r-- 1 root root 385609901 Nov 11 12:27 kitti_train-fold-000-of-002-shard-00008-of-00010\r\n",
      "-rw-r--r-- 1 root root 382992608 Nov 11 12:27 kitti_train-fold-000-of-002-shard-00009-of-00010\r\n",
      "-rw-r--r-- 1 root root     17803 Nov 11 12:27 idx-kitti_train-fold-000-of-002-shard-00006-of-00010\r\n",
      "-rw-r--r-- 1 root root     17806 Nov 11 12:27 idx-kitti_train-fold-000-of-002-shard-00007-of-00010\r\n",
      "-rw-r--r-- 1 root root     17803 Nov 11 12:27 idx-kitti_train-fold-000-of-002-shard-00005-of-00010\r\n",
      "-rw-r--r-- 1 root root     17800 Nov 11 12:27 idx-kitti_train-fold-000-of-002-shard-00002-of-00010\r\n",
      "-rw-r--r-- 1 root root     17801 Nov 11 12:27 idx-kitti_train-fold-000-of-002-shard-00000-of-00010\r\n",
      "-rw-r--r-- 1 root root     17807 Nov 11 12:27 idx-kitti_train-fold-000-of-002-shard-00003-of-00010\r\n",
      "-rw-r--r-- 1 root root     17805 Nov 11 12:27 idx-kitti_train-fold-000-of-002-shard-00001-of-00010\r\n",
      "-rw-r--r-- 1 root root     17818 Nov 11 12:27 idx-kitti_train-fold-000-of-002-shard-00009-of-00010\r\n",
      "-rw-r--r-- 1 root root     17806 Nov 11 12:27 idx-kitti_train-fold-000-of-002-shard-00008-of-00010\r\n",
      "-rw-r--r-- 1 root root     17803 Nov 11 12:27 idx-kitti_train-fold-000-of-002-shard-00004-of-00010\r\n",
      "-rw-r--r-- 1 root root    818535 Nov 11 12:27 kitti_train_20-fold-000-of-002-shard-00000-of-00010\r\n",
      "-rw-r--r-- 1 root root    721418 Nov 11 12:27 kitti_train_20-fold-000-of-002-shard-00001-of-00010\r\n",
      "-rw-r--r-- 1 root root    854477 Nov 11 12:27 kitti_train_20-fold-000-of-002-shard-00002-of-00010\r\n",
      "-rw-r--r-- 1 root root    815046 Nov 11 12:27 kitti_train_20-fold-000-of-002-shard-00003-of-00010\r\n",
      "-rw-r--r-- 1 root root    788750 Nov 11 12:27 kitti_train_20-fold-000-of-002-shard-00004-of-00010\r\n",
      "-rw-r--r-- 1 root root    906215 Nov 11 12:27 kitti_train_20-fold-000-of-002-shard-00005-of-00010\r\n",
      "-rw-r--r-- 1 root root    812471 Nov 11 12:27 kitti_train_20-fold-000-of-002-shard-00006-of-00010\r\n",
      "-rw-r--r-- 1 root root    805662 Nov 11 12:27 kitti_train_20-fold-000-of-002-shard-00007-of-00010\r\n",
      "-rw-r--r-- 1 root root    984752 Nov 11 12:27 kitti_train_20-fold-000-of-002-shard-00008-of-00010\r\n",
      "-rw-r--r-- 1 root root   1059237 Nov 11 12:27 kitti_train_20-fold-000-of-002-shard-00009-of-00010\r\n",
      "-rw-r--r-- 1 root root       241 Nov 11 12:27 idx-kitti_train_20-fold-000-of-002-shard-00004-of-00010\r\n",
      "-rw-r--r-- 1 root root       305 Nov 11 12:27 idx-kitti_train_20-fold-000-of-002-shard-00009-of-00010\r\n",
      "-rw-r--r-- 1 root root       240 Nov 11 12:27 idx-kitti_train_20-fold-000-of-002-shard-00008-of-00010\r\n",
      "-rw-r--r-- 1 root root       240 Nov 11 12:27 idx-kitti_train_20-fold-000-of-002-shard-00007-of-00010\r\n",
      "-rw-r--r-- 1 root root       240 Nov 11 12:27 idx-kitti_train_20-fold-000-of-002-shard-00006-of-00010\r\n",
      "-rw-r--r-- 1 root root       241 Nov 11 12:27 idx-kitti_train_20-fold-000-of-002-shard-00005-of-00010\r\n",
      "-rw-r--r-- 1 root root       240 Nov 11 12:27 idx-kitti_train_20-fold-000-of-002-shard-00003-of-00010\r\n",
      "-rw-r--r-- 1 root root       239 Nov 11 12:27 idx-kitti_train_20-fold-000-of-002-shard-00002-of-00010\r\n",
      "-rw-r--r-- 1 root root       240 Nov 11 12:27 idx-kitti_train_20-fold-000-of-002-shard-00001-of-00010\r\n",
      "-rw-r--r-- 1 root root       240 Nov 11 12:27 idx-kitti_train_20-fold-000-of-002-shard-00000-of-00010\r\n"
     ]
    }
   ],
   "source": [
    "!ls -rlt $LOCAL_DATA_DIR/tfrecords/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4 NGC CLI Instalaltion & Download pre-trained model <a class=\"anchor\" id=\"head-2-4\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NGC CLI to get the pre-trained models. For more details, go to [ngc.nvidia.com](ngc.nvidia.com) and click the SETUP on the navigation bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLI=ngccli_cat_linux.zip\n",
      "--2021-11-11 12:28:07--  https://ngc.nvidia.com/downloads/ngccli_cat_linux.zip\n",
      "Resolving ngc.nvidia.com (ngc.nvidia.com)... 13.249.38.69, 13.249.38.66, 13.249.38.126, ...\n",
      "Connecting to ngc.nvidia.com (ngc.nvidia.com)|13.249.38.69|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 25122135 (24M) [application/zip]\n",
      "Saving to: ‘/home/ubuntu/cv_samples_v1.2.0/retinanet/ngccli/ngccli_cat_linux.zip’\n",
      "\n",
      "ngccli_cat_linux.zi 100%[===================>]  23.96M   102MB/s    in 0.2s    \n",
      "\n",
      "2021-11-11 12:28:07 (102 MB/s) - ‘/home/ubuntu/cv_samples_v1.2.0/retinanet/ngccli/ngccli_cat_linux.zip’ saved [25122135/25122135]\n",
      "\n",
      "Archive:  /home/ubuntu/cv_samples_v1.2.0/retinanet/ngccli/ngccli_cat_linux.zip\n",
      "  inflating: /home/ubuntu/cv_samples_v1.2.0/retinanet/ngccli/ngc  \n",
      " extracting: /home/ubuntu/cv_samples_v1.2.0/retinanet/ngccli/ngc.md5  \n"
     ]
    }
   ],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip -u \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\r\n",
      "| Versi | Accur | Epoch | Batch | GPU   | Memor | File  | Statu | Creat |\r\n",
      "| on    | acy   | s     | Size  | Model | y Foo | Size  | s     | ed    |\r\n",
      "|       |       |       |       |       | tprin |       |       | Date  |\r\n",
      "|       |       |       |       |       | t     |       |       |       |\r\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\r\n",
      "| vgg19 | 77.56 | 80    | 1     | V100  | 153.7 | 153.7 | UPLOA | Aug   |\r\n",
      "|       |       |       |       |       |       | 2 MB  | D_COM | 18,   |\r\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| vgg16 | 77.17 | 80    | 1     | V100  | 113.2 | 113.1 | UPLOA | Aug   |\r\n",
      "|       |       |       |       |       |       | 6 MB  | D_COM | 18,   |\r\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| squee | 65.13 | 80    | 1     | V100  | 6.5   | 6.46  | UPLOA | Aug   |\r\n",
      "| zenet |       |       |       |       |       | MB    | D_COM | 18,   |\r\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| resne | 77.91 | 80    | 1     | V100  | 294.2 | 294.2 | UPLOA | Aug   |\r\n",
      "| t50   |       |       |       |       |       | MB    | D_COM | 18,   |\r\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| resne | 77.04 | 80    | 1     | V100  | 170.7 | 170.6 | UPLOA | Aug   |\r\n",
      "| t34   |       |       |       |       |       | 5 MB  | D_COM | 18,   |\r\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| resne | 76.74 | 80    | 1     | V100  | 89.0  | 88.96 | UPLOA | Aug   |\r\n",
      "| t18   |       |       |       |       |       | MB    | D_COM | 18,   |\r\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| resne | 77.78 | 80    | 1     | V100  | 576.3 | 576.3 | UPLOA | Aug   |\r\n",
      "| t101  |       |       |       |       |       | 3 MB  | D_COM | 18,   |\r\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| resne | 74.38 | 80    | 1     | V100  | 38.3  | 38.31 | UPLOA | Aug   |\r\n",
      "| t10   |       |       |       |       |       | MB    | D_COM | 18,   |\r\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| mobil | 72.75 | 80    | 1     | V100  | 5.0   | 5.01  | UPLOA | Aug   |\r\n",
      "| enet_ |       |       |       |       |       | MB    | D_COM | 18,   |\r\n",
      "| v2    |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| mobil | 79.5  | 80    | 1     | V100  | 26.2  | 26.22 | UPLOA | Aug   |\r\n",
      "| enet_ |       |       |       |       |       | MB    | D_COM | 18,   |\r\n",
      "| v1    |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| googl | 77.11 | 80    | 1     | V100  | 47.6  | 47.64 | UPLOA | Aug   |\r\n",
      "| enet  |       |       |       |       |       | MB    | D_COM | 18,   |\r\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| effic | 77.11 | 80    | 1     | V100  | 26.8  | 26.78 | UPLOA | Aug   |\r\n",
      "| ientn |       |       |       |       |       | MB    | D_COM | 18,   |\r\n",
      "| et_b1 |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| _swis |       |       |       |       |       |       |       |       |\r\n",
      "| h     |       |       |       |       |       |       |       |       |\r\n",
      "| effic | 77.11 | 80    | 1     | V100  | 26.8  | 26.78 | UPLOA | Aug   |\r\n",
      "| ientn |       |       |       |       |       | MB    | D_COM | 18,   |\r\n",
      "| et_b1 |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| _relu |       |       |       |       |       |       |       |       |\r\n",
      "| darkn | 76.44 | 80    | 1     | V100  | 311.7 | 311.6 | UPLOA | Aug   |\r\n",
      "| et53  |       |       |       |       |       | 8 MB  | D_COM | 18,   |\r\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| darkn | 77.52 | 80    | 1     | V100  | 152.8 | 152.8 | UPLOA | Aug   |\r\n",
      "| et19  |       |       |       |       |       | 2 MB  | D_COM | 18,   |\r\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| cspda | 76.44 | 80    | 1     | V100  | 103.0 | 102.9 | UPLOA | Sep   |\r\n",
      "| rknet |       |       |       |       |       | 9 MB  | D_COM | 10,   |\r\n",
      "| 53    |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "| cspda | 77.52 | 80    | 1     | V100  | 62.9  | 62.86 | UPLOA | Sep   |\r\n",
      "| rknet |       |       |       |       |       | MB    | D_COM | 10,   |\r\n",
      "| 19    |       |       |       |       |       |       | PLETE | 2021  |\r\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\r\n"
     ]
    }
   ],
   "source": [
    "!ngc registry model list nvidia/tao/pretrained_object_detection:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/pretrained_resnet18/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 82.38 MB in 6s, Download speed: 13.71 MB/s               \n",
      "----------------------------------------------------\n",
      "Transfer id: pretrained_object_detection_vresnet18 Download status: Completed.\n",
      "Downloaded local path: /home/ubuntu/cv_samples_v1.2.0/retinanet/retinanet/pretrained_resnet18/pretrained_object_detection_vresnet18-4\n",
      "Total files downloaded: 1 \n",
      "Total downloaded size: 82.38 MB\n",
      "Started at: 2021-11-11 12:28:13.086924\n",
      "Completed at: 2021-11-11 12:28:19.099040\n",
      "Duration taken: 6s\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Pull pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/pretrained_object_detection:resnet18 \\\n",
    "                    --dest $LOCAL_EXPERIMENT_DIR/pretrained_resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check that model is downloaded into dir.\n",
      "total 91100\n",
      "-rw------- 1 ubuntu ubuntu 93278448 Oct 22 14:12 resnet_18.hdf5\n"
     ]
    }
   ],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $LOCAL_EXPERIMENT_DIR/pretrained_resnet18/pretrained_object_detection_vresnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed: 42\r\n",
      "retinanet_config {\r\n",
      "  aspect_ratios_global: \"[1.0, 2.0, 0.5]\"\r\n",
      "  scales: \"[0.045, 0.09, 0.2, 0.4, 0.55, 0.7]\"\r\n",
      "  two_boxes_for_ar1: false\r\n",
      "  clip_boxes: false\r\n",
      "  loss_loc_weight: 0.8\r\n",
      "  focal_loss_alpha: 0.25\r\n",
      "  focal_loss_gamma: 2.0\r\n",
      "  variances: \"[0.1, 0.1, 0.2, 0.2]\"\r\n",
      "  arch: \"resnet\"\r\n",
      "  nlayers: 18\r\n",
      "  n_kernels: 1\r\n",
      "  n_anchor_levels: 1\r\n",
      "  feature_size: 256\r\n",
      "  freeze_bn: False\r\n",
      "  freeze_blocks: 0\r\n",
      "}\r\n",
      "training_config {\r\n",
      "  enable_qat: False\r\n",
      "  pretrain_model_path: \"/workspace/tao-experiments/retinanet/pretrained_resnet18/pretrained_object_detection_vresnet18/resnet_18.hdf5\"\r\n",
      "  batch_size_per_gpu: 8\r\n",
      "  num_epochs: 100\r\n",
      "  n_workers: 2\r\n",
      "  checkpoint_interval: 10\r\n",
      "  learning_rate {\r\n",
      "    soft_start_annealing_schedule {\r\n",
      "      min_learning_rate: 4e-5\r\n",
      "      max_learning_rate: 1.5e-2\r\n",
      "      soft_start: 0.1\r\n",
      "      annealing: 0.3\r\n",
      "    }\r\n",
      "  }\r\n",
      "  regularizer {\r\n",
      "    type: L1\r\n",
      "    weight: 2e-5\r\n",
      "  }\r\n",
      "  optimizer {\r\n",
      "    sgd {\r\n",
      "      momentum: 0.9\r\n",
      "      nesterov: True\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "eval_config {\r\n",
      "  validation_period_during_training: 10\r\n",
      "  average_precision_mode: SAMPLE\r\n",
      "  batch_size: 8\r\n",
      "  matching_iou_threshold: 0.5\r\n",
      "}\r\n",
      "nms_config {\r\n",
      "  confidence_threshold: 0.01\r\n",
      "  clustering_iou_threshold: 0.6\r\n",
      "  top_k: 200\r\n",
      "}\r\n",
      "augmentation_config {\r\n",
      "    output_width: 512\r\n",
      "    output_height: 512\r\n",
      "    output_channel: 3\r\n",
      "}\r\n",
      "dataset_config {\r\n",
      "  data_sources: {\r\n",
      "    tfrecords_path: \"/workspace/tao-experiments/data/tfrecords/kitti_train*\"\r\n",
      "  }\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"screw1\"\r\n",
      "      value: \"screw1\"\r\n",
      "  }\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"screw3\"\r\n",
      "      value: \"screw3\"\r\n",
      "  }\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"screw6\"\r\n",
      "      value: \"screw6\"\r\n",
      "  }\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"screw8\"\r\n",
      "      value: \"screw8\"\r\n",
      "  }\r\n",
      "   validation_data_sources: {\r\n",
      "    image_directory_path: \"/workspace/tao-experiments/data/synthetic_test/images\"\r\n",
      "    label_directory_path: \"/workspace/tao-experiments/data/synthetic_test/labels\"\r\n",
      "  } \r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat $LOCAL_SPECS_DIR/retinanet_train_resnet18_kitti.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_unpruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Run TAO training on Synthetic Dataset<a class=\"anchor\" id=\"head-4\"></a>\n",
    " * Provide the sample spec file for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run with multigpu, please change --gpus based on the number of available GPUs in your machine.\n",
      "2021-11-11 12:28:20,247 [INFO] root: Registry: ['nvcr.io']\n",
      "2021-11-11 12:28:20,328 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/ubuntu/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "2021-11-11 12:30:28,725 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "!sed -i \"s|YOUR_PRETRAINED_MODEL|$USER_EXPERIMENT_DIR/pretrained_resnet18/pretrained_object_detection_vresnet18/resnet_18.hdf5|g\" $LOCAL_SPECS_DIR/retinanet_train_resnet18_kitti.txt\n",
    "print(\"To run with multigpu, please change --gpus based on the number of available GPUs in your machine.\")\n",
    "!tao retinanet train -e $SPECS_DIR/retinanet_train_resnet18_kitti.txt \\\n",
    "                     -r $USER_EXPERIMENT_DIR/experiment_dir_unpruned \\\n",
    "                     -k $KEY \\\n",
    "                     --gpus 1 --use_amp > out_resnet18_synth_amp16.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_unpruned_fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Run TAO training or fine tuning on Real Dataset<a class=\"anchor\" id=\"head-4\"></a>\n",
    " * Provide the sample spec file for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i \"s|YOUR_PRETRAINED_MODEL|$USER_EXPERIMENT_DIR/pretrained_resnet18/pretrained_object_detection_vresnet18/resnet_18.hdf5|g\" $LOCAL_SPECS_DIR/retinanet_train_resnet18_kitti.txt\n",
    "print(\"To run with multigpu, please change --gpus based on the number of available GPUs in your machine.\")\n",
    "!tao retinanet train -e $SPECS_DIR/retinanet_train_fine_resnet18_kitti.txt \\\n",
    "                     -r $USER_EXPERIMENT_DIR/experiment_dir_unpruned_fine \\\n",
    "                     -k $KEY \\\n",
    "                     --gpus 1 --use_amp > out_resnet18_synth_fine_amp16.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate trained models <a class=\"anchor\" id=\"head-5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao retinanet evaluate -e $SPECS_DIR/retinanet_train_fine_resnet18_kitti.txt \\\n",
    "                        -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned_fine/weights/retinanet_resnet18_epoch_100.tlt \\\n",
    "                        -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize inferences <a class=\"anchor\" id=\"head-9\"></a>\n",
    "In this section, we run the tlt-infer tool to generate inferences on the trained models and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Running inference for detection on n images\n",
    "!tao retinanet inference -i $DATA_DOWNLOAD_DIR/real_test/images \\\n",
    "                         -o $USER_EXPERIMENT_DIR/retinanet_infer_images \\\n",
    "                         -e $SPECS_DIR/retinanet_train_fine_resnet18_kitti.txt \\\n",
    "                         -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned_fine/weights/retinanet_resnet18_epoch_100.tlt \\\n",
    "                         -l $USER_EXPERIMENT_DIR/retinanet_infer_labels \\\n",
    "                         -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inference` tool produces two outputs. \n",
    "1. Overlain images in `$LOCAL_EXPERIMENT_DIR/retinanet_annotated_images`\n",
    "2. Frame by frame bbox labels in kitti format located in `$LOCAL_EXPERIMENT_DIR/retinanet_infer_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "!pip3 install matplotlib==3.3.3\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n",
    "\n",
    "def visualize_images(image_dir, num_cols=1, num_images=1):\n",
    "    output_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], image_dir)\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "OUTPUT_PATH = 'retinanet_infer_images' # relative path from $LOCAL_EXPERIMENT_DIR.\n",
    "COLS = 2 # number of columns in the visualizer grid.\n",
    "IMAGES = 4 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
